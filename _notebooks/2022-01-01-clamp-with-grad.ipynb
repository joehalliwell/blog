{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clamp-with-grad.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# An analysis of `clamp_with_grad()` \n",
        "\n",
        "This notebook examines the `clamp_with_grad()` function that's sometimes used in the CLIP-guided art community.\n",
        "\n",
        "I wanted to build some intuition about how the backward pass is defined and why the function works.\n"
      ],
      "metadata": {
        "id": "5X0m-Jf_UdPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Setup\n",
        "\n",
        "Import the usual libraries and tweak the matplotlib setup"
      ],
      "metadata": {
        "id": "UHHMgFS4V-19"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtlEMPrGzdAO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from math import pi\n",
        "\n",
        "matplotlib.rcParams['figure.figsize'] = (8 * 3, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a helper function to access the gradient"
      ],
      "metadata": {
        "id": "BGjNxmTxUuNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gradient(f, xs):\n",
        "  xs = xs.detach().clone()\n",
        "  xs.requires_grad = True\n",
        "\n",
        "  ys = f(xs)\n",
        "  ys.backward(torch.ones_like(ys))  \n",
        "  return xs.grad.detach()\n",
        "\n",
        "\n",
        "x = torch.arange(-2, 2, 0.01)\n",
        "\n",
        "plt.plot(x, torch.sin(x * 5), label=\"sin(x)\")\n",
        "plt.plot(x, get_gradient(torch.sin, x * 5), label=\"dsin(x)/dx\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "euUR8bvJ3sSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define `clamp_with_grad()`"
      ],
      "metadata": {
        "id": "-8sX46HHWLO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return (\n",
        "            grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0),\n",
        "            None,\n",
        "            None,\n",
        "        )\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "\n",
        "class ClampWithGrad2(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        clamped = input.clamp(ctx.min, ctx.max)\n",
        "        delta = input - clamped\n",
        "        v = torch.where(delta == 0, grad_in, torch.sign(delta))\n",
        "        return (v, None, None)\n",
        "\n",
        "clamp_with_grad2 = ClampWithGrad2.apply\n",
        "\n"
      ],
      "metadata": {
        "id": "16Dmf7dsUr0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(fn, ax, title):\n",
        "  x = torch.arange(-2, 2, 0.01)\n",
        "  clamp = lambda x: fn(x, -1, 1)\n",
        "  \n",
        "  #loss = lambda x: (clamp(x)) ** 2\n",
        "  loss = lambda x: torch.sin(clamp(x) * pi) + 1\n",
        "\n",
        "  ax.plot(x, clamp(x), label=\"clamp\", alpha=0.5)\n",
        "  ax.plot(x, loss(x), label=\"loss\", alpha=0.5)\n",
        "  \n",
        "  #ax.plot(x, get_gradient(lambda x: 2 * x, clamp(x)), label=\"dloss/dclamp\" )\n",
        "  #ax.plot(x, get_gradient(lambda x: torch.sin(x * pi), clamp(x)), label=\"dloss/dclamp\", alpha=0.5)\n",
        "  \n",
        "  #ax.plot(x, get_gradient(clamp, x), label=\"dclamp/dx\", alpha=0.5)\n",
        "  ax.plot(x, get_gradient(loss, x), label=\"dloss/dx\", alpha=0.5)\n",
        "  ax.legend()\n",
        "  ax.title.set_text(title)\n",
        "\n",
        "fig, ax = plt.subplots(ncols=3)\n",
        "plot(torch.clamp, ax[0], \"clamp\")\n",
        "plot(clamp_with_grad, ax[1], \"clamp_with_grad\")\n",
        "plot(clamp_with_grad2, ax[2], \"clamp_with_grad_proposed\")\n"
      ],
      "metadata": {
        "id": "8hmk6wxP0dRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ls2oUma4BzH0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}