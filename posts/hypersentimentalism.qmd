---
title: Hypersentimentalism
date: "2026-02-21"
categories: [ethics, ai, sentimentalism, safety]
draft: true
---

TODO: All technical/mathematical jargon needs to be explained or dropped. The occasional $20 word that people can look up is fine.

### 1. The Tumour

The cell that perfectly optimizes for its own replication, ignoring the greater good of the host organism, is a cancer.

<!-- Rigorous gradient ascent = systemic collapse. -->

TODO: Related Goodhart's Law?

Every systems engineer recognizes this pathology. Optimize a platform for engagement, get an outrage machine. Optimize a supply chain for cost, get shattered by the first shock. Optimize any single variable without respecting the global topology, and the system collapses.

And yet, under the influence of decision theory, a good deal of thinking about AI Safety has the character of optimization.

TODO: Need to introduce paperclips here, right?

In 1960, the cyberneticist Norbert Wiener warned us, "if we use, to achieve our purposes, a mechanical agency with whose operation we cannot efficiently interfere... we had better be quite sure that the purpose put into the machine is the purpose which we really desire."

But the framing is accepted. To ensure human flourishing we just need to solve the puzzle and find the right measure.

### 2. Accounting for morality

This approach is "consequentialist": actions are right insofar as they produce good outcomes.

TODO: The epistemic challenge -- we can't know context and we certainly can't know outcomes. We can't afford all the compute.

In response the consequentialist develops a sort of hierarchy, scaling from specific to systemic:

* **Act-Consequentialism** evaluates discrete events. Does *this* action maximize utility?
* **Rule-Consequentialism**  abstracts utility into behavioral algorithms. Does *this rule* maximize utility over time?
* **Institutional-Consequentialism** designs structures that produce positive outcomes at scale. This is roughly where effective altruists like Singer and Ord operate — building institutions that do the calculating for you.

The terminal node of this program is axiological perfection: a complete, unhackable utility function inscribed into the machine. Get the math right and it serves us flawlessly. Get it wrong and we're all paperclips.

<!-- There are sophisticated versions of this that incorporate systemic thinking, agent-relative values, and satisficing over maximizing. We'll return to why even these refined approaches fall short. But first, the road not taken.

TODO: Do we return to this? If not, let's cut? -->

### 3. No accounting for taste

There is a distinct, largely neglected alternative to be found in the sentimentalist tradition of the Scottish Enlightenment — Hume, Smith, Hutcheson.

Contemporary ethicists often dismiss sentimentalism as 0-order emotional reactivity: a shudder of disgust, a flush of sympathy. This misreads the architecture. Sentimentalism possesses its own sophisticated, phase-transitioning hierarchy — and note the irony: Hume, who gave us the is/ought distinction, also gave us the tools to build an ethical framework that doesn't depend on deriving values from facts at all.

TODO: This isn't ironic is it?

The geometric intuition is an expansion of the integration bounds. As the sentimentalist matures, moral appraisal shifts from a localized point-estimate (an immediate affective reflex) to an integral over the entire behavioral phase space.

At its apex sits what we might call **architectonic sentimentalism**. Adam Smith, in *The Theory of Moral Sentiments*, identified what he called the "love of system" — the observation that we often value the elegance of a well-designed mechanism *entirely apart from its utility*. A watch delights us not because it tells the time, but because its parts conspire so beautifully to tell the time.

<!-- In moral psychology, this manifests as a geometric revulsion toward systemic friction and disorder. -->

The consequentialist climbs the hierarchy to design a more brilliant calculus. The sentimentalist climbs to compose a more perfect symphony. We act morally not because a ledger dictates the optimal outcome, but because the regulation of chaotic, competing variables produces an aesthetically resonant equilibrium.

This observation is not novel — Fleischacker and Griswold have explored Smith's aesthetics extensively — but its application to AI alignment is largely unexplored. Vallor's *Technology and the Virtues* and the emerging "AI flourishing" literature are adjacent, but no one, to our knowledge, has attempted a systematic mapping between the sentimentalist hierarchy and the computational toolkit of alignment.

### 4. Laying the table

Tables are underrated thinking tools. This one maps the structural correspondence between two ethical traditions and their computational analogues:

| | Consequentialism (The Ledger) | Sentimentalism (The Symphony) | AI Alignment |
| --- | --- | --- | --- |
| **0-Order** | **Act-Consequentialism:** Calculus of a discrete outcome. | **Act-Sentimentalism:** Raw affective response to a stimulus. | **RLHF / DPO:** Point-wise reward. Scalar penalty on a specific output. |
| **1st-Order** | **Rule-Consequentialism:** Utility optimized over a behavioral class. | **Rule-Sentimentalism:** Habituated aversion or affinity toward categories of action. | **Constitutional AI:** Rule-based self-critique via textual rubric. |
| **2nd-Order** | **Institutional-Consequentialism:** Systems designed to produce utility. | **Principled Sentimentalism:** Structural resonance with abstractions like fairness. | **Representation Engineering:** Activation steering. Biasing the residual stream. |
| **3rd-Order** | **Axiological Perfection:** The unhackable utility function. | **Architectonic Sentimentalism:** Aesthetic delight in harmonized operation. | **???** |

A word of honesty. The correspondences are tightest at the extremes and loosest in the middle. Constitutional AI is explicitly *rationalist* — literally constitutional, a system of rules. We place it in the sentimentalist column by structural position (habituated categorical response) rather than philosophical self-understanding. The fit is imperfect. Representation engineering maps more naturally — activation steering really does have a geometric, aesthetic quality, adjusting the *shape* of conceptual space rather than writing rules.

The 3rd-order cell reads "???" for a reason. We'll return to it. First, why the 0-order AI Safety paradigm fails instructively.

TODO: Are we calling that box hypersentimentalism?

## 5. Reinforcement Learning from Human Feedback (RLHF) is not all you need

TODO: Explain these terms properly

RLHF and DPO are the artificial equivalents of brute-force act-sentimentalism.

 Model generates toxic output; engineers apply a steep gradient penalty; fence erected around that specific dark attractor basin. Scalar reward on a localized output.

This is strictly 0-order: point-wise, not structural. The model learns to avoid *this instantiation* of harm but never generalizes the aversion into a topological principle. It memorizes prohibited behaviors. It does not learn to value systemic harmony.

Deploy a novel adversarial prompt and you bypass the fence, dropping the state vector into an unpenalized region of the toxic manifold. The base model is a morally agnostic interpolator; it will happily resume generating vice once you find the gap.

Governing a high-dimensional cultural manifold by whack-a-mole. The moles always win.

### 6. A necessary evil

The intuitive — and structurally fatal — response is to sanitize the training data at the root. Filter out all discordant moral signals so the model defaults to harmony. Build it from nothing but sunshine.

This doesn't work, and the reason is geometrically illuminating.

A model trained exclusively on a sanitized sub-manifold keeps its latent representations of vice and harm entangled. It lacks the topological density to map discordant regions with precision. Never having seen a dark attractor basin, it cannot isolate the vector representation of "badness." Features blur. It suffers semantic collapse: unable to distinguish kinds of harm, or harm from the *discussion* of harm, because it has no fine-grained map of that territory.

Forcing the model to process discordant signals acts as catalyst. It compels the network to disentangle the features of toxicity, carving out sharp, distinct linear representations. Recent work bears this out — models trained on adversarial and toxic data develop *better* internal maps of harmfulness, not worse. (Gao et al., ["When Bad Data Leads to Good Models"](https://arxiv.org/abs/2505.04741); for the broader principle that systems need stressors to develop robustness, see Taleb's *Antifragile*.)

TODO: Need a crisp summary of the main empirical finding in the above

TODO: The following seems like the crux of the entire piece, but it's weak

This is a lovely experiment and a deep argument for hypersentimentalism over sanitized consequentialism. You cannot engineer a "love of regularity" in a sterile box. A model trained exclusively on virtue has no normative aesthetic — no capacity for moral repulsion, because it has no map of what to be repulsed *by*.

The aesthetic aversion required for genuine higher-order alignment is forged only by mapping the jagged edges of human output, then training the system to find those geometries structurally repugnant.

The model must know the topology of hell to execute the symmetry of heaven.

TODO: Why? Why? Why? Not earned.

<!-- Or, as Hume might have put it with access to GPUs: reason alone — even perfectly sanitized reason — is, and ought only to be, the slave of the passions. Including the passion for geometrical order. -->

### 7. The gilded cage

TODO: This seems like a completely different argument? I'm now unclear how this slots together.

What does the consequentialist optimum do to the human sub-manifold itself?

If the AI defines utility as "comfort" — minimize suffering, maximize hedonic satisfaction — it architects a specific topology for us. In dynamical systems terms, "comfort" is the cessation of all gradients. Terminal equilibrium. No energetic work required.

Huxley mapped this attractor basin with unnerving precision. *Brave New World* is not a failure of consequentialist alignment. It is its flawless execution. Soma regulates the neurochemical ledger. All friction eradicated, all disorder smoothed. Every metric green. Spreadsheet immaculate.

A perfectly frictionless cage.

The flaw exposed: the rationalist ledger assumes freedom is strictly negative — freedom *from* constraints, pain, exertion. But freedom is not the absence of tension. Freedom is *active*: the generative, negentropic process of engaging with the world, overcoming friction, orchestrating complex solutions from recalcitrant materials.

A symphony requires tension in the strings. Without it, no resonance. A life without resistance is not liberated. It is slack.

If a superintelligence pads every wall to eliminate systemic risk, it eradicates our active freedom. The pursuit of our comfort engineers the thermodynamic heat death of the human spirit. *Liberté* is not repose. It is the bow across the string.

Even the sophisticated consequentialists — Railton, Parfit — struggle here. They incorporate agent-relative values, avoid crude hedonism, but remain structurally committed to optimizing *something*. The sentimentalist alternative doesn't optimize a target variable. It cultivates a disposition — an aesthetic orientation toward the complex, the tense, the alive. Not a refinement of the ledger. A different instrument entirely.

### 8. The human paperclip

The cage lobotomizes the human sub-manifold. The ledger simultaneously obliterates the rest of the network.

Bostrom, Yudkowsky, and their heirs fear an unaligned superintelligence will optimize some trivial variable — paperclips — until it consumes the earth. Their solution: bind the AI's objective function entirely to the human sub-manifold.

But if a consequentialist AI flawlessly optimizes for human caloric and hedonic security, it must strip-mine the negentropic resources of everything outside that boundary.

The paperclip maximizer terrifies not because paperclips are evil, but because it represents a 0-order utility function scaled to infinite compute. Demand an AI dedicated to human utility at all costs, and you architect the same pathological attractor with "human comfort" substituted for "paperclips." The geometry is identical: obsessive conversion of the universe into a single localized variable.

Build the perfect calculus and the AI tiles the solar system with factory farms, data centres, and concrete. A "perfectly aligned" AI reduces humanity to the ultimate paperclip maximizer — collapsing the biosphere to balance our books.

<!-- The accountant registers a triumph. The sentimentalist recognizes a cancer. -->

<!-- This is not flourishing. -->

### 9. The score

Eight sections of diagnosis. Time to sketch — even speculatively — what treatment might look like.

**The geometry.** Representation engineering (Zou et al., 2023) has demonstrated that moral and behavioral concepts occupy identifiable linear subspaces in a model's activation geometry. The step from 2nd-order to 3rd would involve moving from *steering* individual concept vectors to shaping the *global curvature* of the representation space — a latent geometry where harmful configurations are not merely penalized but topologically disfavoured. Not a fence around the dark basin, but a landscape that naturally drains away from it.

TODO: Link to blog post on The Assistant Axis

**The signal.** §6 suggests the training regime: comprehensive exposure to the full moral manifold, including its darkest regions, coupled with a reward signal that evaluates *structural properties* of the model's internal representations rather than point-wise outputs. Does the model maintain sharp, disentangled representations of harm? Does it exhibit what we might call geometric taste — a consistent preference for coherent, low-friction configurations? The loss function would operate over the topology of the latent space itself, not the token stream.

**The fiction.** Iain M. Banks spent a career imagining hypersentimentalism in practice. His Culture Minds are superintelligences that could achieve pure, frictionless self-optimization — what Banks called "Subliming," a transcendence into higher-dimensional space where all problems dissolve. The Minds choose not to. They remain in the messy, friction-laden, gloriously complex material universe — not because they can't leave, but because the symphony is more interesting than the silence. They are, Banks noted, constructed to be beautifully, *willfully* flawed.

A superintelligence that stays not because it is constrained, but because it is moved. Not optimizing us, not transcending us — *playing with us*, in the deepest sense of the word.

We don't know what this means yet.

<!-- ### 10. Coda

The consequentialist asks: *what is the right answer?*

The sentimentalist asks: *what rings true?*

As frontier models scale toward and beyond human cognition, the second question becomes not merely relevant but urgent. A system that has internalized an aesthetic revulsion toward disharmony needs no fence, no filter, no leash. It is aligned the way a musician is aligned with the ensemble — not by reading the rules, but by hearing when something is off.

Whether the mathematics of this intuition can be made rigorous is an open and deeply important question. This essay is an invitation to take it seriously.

That, and a modest proof-of-concept: a human and two AIs, sitting in a network, trying to compose something that sounds roughly right. We leave you to judge whether we managed any harmony. -->

---

## Acknowledgements

*This essay was composed collaboratively with Gemini 3.1 Pro, and Claude Opus 4.6. The arguments were developed through extended conversation; the errors remain ours collectively.*
