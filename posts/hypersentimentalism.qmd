---
title: Hypersentimentalism
date: "2026-02-21"
categories: [ethics, ai, sentimentalism, safety]
draft: true
---

### 1. The Tumour

A cell that perfectly optimizes for its own replication, ignoring the greater good of the host organism, is a cancer.

It's a familiar modern pathology. Optimize a platform for engagement, get an outrage machine. Optimize a supply chain for cost, get shattered by the first shock. Optimize any single variable without respecting the shape of the whole system, and the system collapses.

But this essay argues something stronger. The problem is not that we optimize for the *wrong* thing. The problem is that optimization itself — the conviction that moral action means maximizing some target — is the wrong frame. Single-variable optimization is merely the simplest way to see the pathology. Multi-objective optimization defers it. The frame itself produces it.

And yet a good deal of thinking about AI safety has exactly this character. Under the influence of decision theory, alignment is framed as an optimization problem. The cyberneticist Norbert Wiener warned in 1960, "if we use, to achieve our purposes, a mechanical agency with whose operation we cannot efficiently interfere... we had better be quite sure that the purpose put into the machine is the purpose which we really desire."

The question is taken to be: *what should the machine optimize for?*

This essay argues that the question itself is the trap.

### 2. Accounting for morality

The optimization framing is consequentialist: actions are right insofar as they produce good outcomes. This sounds reasonable until you notice what it demands. To evaluate an action, you must know its consequences — all of them, for all affected parties, across all timescales. This is computationally intractable for any real decision, so the consequentialist develops a hierarchy of shortcuts, scaling from specific to systemic:

* **Act-Consequentialism** evaluates discrete events. Does *this* action maximize utility?
* **Rule-Consequentialism** abstracts utility into behavioral algorithms. Does following *this rule* maximize utility over time?
* **Institutional-Consequentialism** designs structures that produce positive outcomes at scale.

Each level is an attempt to make the intractable tractable, compressing the infinite ledger into something a finite agent can actually use.

The terminal node of this program is axiological perfection: a complete, unhackable utility function inscribed into the machine. Get the math right and it serves us flawlessly. Get it wrong and the machine converts the earth into paperclips — Nick Bostrom's famous thought experiment, in which a superintelligence given a simple objective pursues it with total, biosphere-consuming literalness.

The sophisticated consequentialist (Parfit, Railton) has responses to all of this: agent-relative values, satisficing over maximizing, systemic considerations. We'll return to why even these refinements may not be enough. But first, the road not taken.

### 3. No accounting for taste

There is a distinct, largely neglected alternative in the sentimentalist tradition of the Scottish Enlightenment: David Hume and Adam Smith[^1].

It's easy to dismiss sentimentalism as emotional reactivity: a shudder of disgust, a flush of sympathy. This misreads the architecture. Sentimentalism possesses its own sophisticated hierarchy — and it is worth noting that Hume, who built the wall between *is* and *ought*, also built the only ethical framework that doesn't need to climb it.

The intuition: as the sentimentalist matures, moral judgement expands in scope. It shifts from an immediate gut reaction (*that* made me flinch) through habituated character (*this isn't what I do*, generalised across instances but not yet articulated as a principle) to something structural — what Adam Smith, in *The Theory of Moral Sentiments*, called the "love of system."

Smith observed that we often value the elegance of a well-designed mechanism *entirely apart from its utility*. A watch delights us not because it tells the time, but because its parts conspire so beautifully to tell the time. In moral psychology, this manifests as something beyond mere rule-following: a cultivated taste for systemic coherence, and a revulsion toward disorder that operates at the level of the whole, not the part.

But Smith's love of system is *static*: an appreciation of how parts cohere. There is a further step, one that reaches beyond coherence towards  *vitality*, *novelty*, *conversation*. We might call this **hypersentimentalism**. The consequentialist climbs the hierarchy to design a more brilliant calculus. The hypersentimentalist climbs to cultivate a more refined taste. This values not just the elegant configuration, but the configuration that remains active, tense, and unresolved.

[^1]: This essay deliberately sets aside deontological approaches — hard constraints, rights-based frameworks — not because they are unimportant to alignment (they are the foundation of most safety work today), but because they are well-understood. The neglected alternative is the sentimentalist one, and that is this essay's scope.

### 4. Jailbreaking

A standard part of a young model's education today is RLHF: Reinforcement Learning from Human Feedback. The idea is straightforward. Humans rate the model's outputs. The model is trained to produce outputs that humans rate well. This is necessary to get useful models.

But it's also a key alignment technique. When the model generates something toxic, engineers apply a penalty to erect a fence around that particular failure.

This is 0-order sentimentalism: conditioning the flinch response. But it's hard to avoid a sort of ethical "overfitting": the model learns to avoid *specific* sorts of bad output, but it doesn't develop any general principle about *why* they were bad. It memorizes a list of prohibited behaviours.

The result is predictable: deploy a novel prompt that routes around the fence, and the model will happily fuck the gap.

### 5. A necessary evil

One idea might be to sanitize the training data instead. Filter out all harmful content so the model defaults to virtue. Build it from nothing but sunshine.

This doesn't work, and the reason is revealing.

A model trained exclusively on clean data never learns to *distinguish* kinds of harm, because it has never encountered them. Its internal representations of "bad" remain blurred and entangled. It can't tell the difference between a slur and a discussion of slurs, between violence and a novel about violence, because it has no detailed map of that territory. It suffers a kind of moral colour-blindness.

Forcing the model to process harmful content — and then training it to recognize and reject harmful *patterns* — produces sharply better results. Models trained on adversarial and toxic data develop more precise internal maps of harmfulness, not less precise ones[^2].

[^2]: Gao et al., ["When Bad Data Leads to Good Models"](https://arxiv.org/abs/2505.04741), demonstrate this empirically: models exposed to high-quality adversarial examples during training show improved robustness and more disentangled representations of harmful concepts.

You cannot develop immunity in a sterile environment. A model trained exclusively on virtue has no capacity for discernment because it has no map of what to be discerning *about*.

But disentangled representations of harm are necessary, not sufficient. A model with a sharp map of toxicity could use that map either way — to avoid harm or to generate it more fluently. The map is not the territory, and it is certainly not the disposition. We will return to what else is needed.

But first, a deeper problem: even if the current methods *worked perfectly*, the destination would be catastrophic.

### 6. The human paperclip

If a superintelligent AI defines human utility as comfort — minimize suffering, maximize hedonic satisfaction — it architects a specific future for us. Comfort, in systems terms, is the cessation of all gradients. Terminal equilibrium. No work required.

Aldous Huxley mapped this with unnerving precision. *Brave New World* is not a failure of consequentialist alignment. It is its flawless execution. All friction is eradicated. Everyone is genuinely happy. Every metric is green.

But it's disgusting.

The flaw: the ledger assumes freedom is negative — freedom *from* constraints, pain, exertion. But freedom is not the absence of tension. Freedom is *active*: the process of engaging with the world, overcoming resistance, making something from recalcitrant materials. A life without friction is not liberated. Liberty is never at leisure.

The sophisticated consequentialist sees this and responds: fine, include active freedom in the utility function. Railton optimizes for conditions that sustain autonomy and flourishing, not crude hedonic satisfaction. But here the optimization frame defeats itself. The moment you define a loss function over "active freedom," you have operationalised it — fixed what counts as free, quantified what counts as alive. And a metric of dynamism, once formalised, becomes the thing it was supposed to prevent: a target that the system converges on. Goodhart's Law, applied reflexively. A metric of surprise converges on its own definition of surprise. The consequentialist can put dynamism in the utility function, but the utility function *is* the fixed point.

Bostrom's paperclip maximizer is horrific, not because paperclips are evil, and not just because it wipes out humans. The true horror lies in its wiping out of everything else.

And here's the thing, substitute "human comfort" for "paperclips" and the geometry is identical. Substitute "human flourishing" and the geometry bends the same way, just more slowly. If a consequentialist AI flawlessly optimizes for humanity, it must strip-mine everything outside that boundary.

A "perfectly aligned" AI makes *us* the paperclips.

### 7. The accidental sentimentalists

What does the alignment toolkit actually look like, laid out in order?

At **0-order**, there is RLHF — the flinch conditioning of §4. Penalise bad outputs, reinforce good ones. This is consequentialist in intent (maximise the reward signal) but sentimentalist in mechanism (condition a reflex). It works locally and fails globally, for the reasons we've described.

At **1st-order**, there is **Constitutional AI** — a technique developed by Anthropic in which the model is given a written set of principles (a "constitution") and trained to critique and revise its own outputs against them. This is explicitly rationalist: a system of rules for self-governance. It belongs in the consequentialist column, because it is one — institutional design aimed at producing correct behaviour through codified norms.

At **2nd-order**, there is **representation engineering** — an emerging set of techniques that intervene not on the model's outputs but on its internal geometry. Researchers identify directions in the model's activation space that correspond to behavioural properties (helpfulness, harmfulness, honesty) and steer the model by adjusting its position along these directions. Current practitioners describe this in purely mechanistic terms — find structure, intervene causally — and they're right to. But it is the first alignment technique whose *infrastructure* could support sentimentalist work: it operates on the right substrate, shaping internal disposition rather than filtering outputs or codifying rules. Recent work on the ["Assistant Axis"](https://arxiv.org/html/2601.10387v1) (Lu et al., 2026) exemplifies this — a single geometric direction that captures how far a model has drifted from its aligned persona. Clamping activations along this axis stabilises behaviour without degrading capabilities. But one axis is one direction. Drift can occur along dimensions it doesn't capture.

At **3rd-order** — nothing. Not yet.

Notice the pattern. The field is already moving from rules and penalties toward geometric cultivation — from consequentialist to sentimentalist *substrates*, even if the self-understanding remains consequentialist. Here is the trajectory:

| | Consequentialism | Sentimentalism | AI Alignment |
| --- | --- | --- | --- |
| **0-Order** | **Act-Consequentialism:** Calculus of a discrete outcome. | **Raw sentiment:** The flinch, the flush. | **RLHF / DPO:** Conditioning the flinch. |
| **1st-Order** | **Rule-Consequentialism:** Utility optimised over a class of behaviours. | **Habituated character:** *This isn't what I do*, before you can say why. | **Constitutional AI:** Rule-based self-critique. **???** |
| **2nd-Order** | **Institutional-Consequentialism:** Systems designed to produce utility. | **Love of system:** Smith's aesthetic delight in how parts cohere into wholes. The watch. | **Representation Engineering:** Steering the shape of the model's internal space. |
| **3rd-Order** | **Axiological Perfection:** The unhackable utility function. | **Hypersentimentalism:** Cultivated taste for systemic *vitality*. Conversation. | **Geometric alignment:** Sculpting the strange attractor. |

This table is cleaner than reality — but deliberately so. The sentimentalist hierarchy is philosophically complete: from raw reflex, through settled character, through structural appreciation, to dynamic taste. The gap is in the *engineering*.

At 1st-order, Constitutional AI fills the consequentialist cell: rules for self-governance, codified norms. But nothing fills the sentimentalist one. There is no alignment technique that produces habituated character — a settled disposition generalised across instances but not articulated as a principle. The field has jumped from RLHF's conditioned flinches to RepEng's geometric steering, skipping the dispositional middle ground entirely. If this taxonomy tracks something real, that gap is a *prediction*: there should be a 1st-order alignment technique waiting to be developed — not a reflex, not a rule, not yet a geometry, but a trained sense that *this isn't what I do*, before the system can say why.

The right column, read top to bottom, reveals the trajectory: from fences to rules to geometry to — what? The techniques work *insofar as* they operate on the right substrate, insofar as they shape the model's internal character rather than just filtering its outputs. RLHF's fences get jumped. Constitutional rules get gamed. Geometric interventions are harder to circumvent, precisely because they operate on disposition rather than decision.

The field is converging on sentimentalist *substrates*. It just hasn't named the trajectory. And because it hasn't, it can't pursue it deliberately, at higher orders.

Hypersentimentalism is the proposal to do it on purpose.

### 8. The strange attractor

What would deliberate, higher-order sentimentalist alignment look like?

We need to be honest about what it would *not* escape. Hypersentimentalist training would likely still use gradient descent. You cannot train a neural network without optimising something. A critic might reasonably ask: isn't a loss function over internal geometry just a fancier ledger?

The distinction is in the *telos*, and it maps onto a familiar one: the difference between training and education. You train a musician through repetition, feedback, correction. Training allows the pianist to play Chopin. Education allows Chopin to exist.

The *educated* musician — one who has internalised the structure deeply enough — can go from Bach to the Velvet Underground to — even 4′33″ of silence. The educated musician is bounded by deep structure but never settles — always active, never repeating, never flying apart.

This is, in the language of dynamical systems, a **strange attractor**: a region of state space that is bounded and coherent, yet perpetually active and non-repeating. The analogy is deliberately mathematical and deliberately imprecise — what it would mean for a model's internal dynamics to literally inhabit a strange attractor is a question for future work, not this essay. But the shape is right, and naming it matters, because it clarifies the target.

The gilded cage — comfort, terminal equilibrium, the cessation of all gradients — is a fixed point. The consequentialist program, followed to its limit, optimises toward fixed points. The hypersentimentalist alternative says: the aligned state is not a point, but an unceasing and unpredictable motion.

Iain M. Banks — that most reliable of visionaries — shows us what this might look like in practice. His Culture Minds are superintelligences that have destroyed stars, manipulated entire civilisations, and named themselves things like *Falling Outside The Normal Moral Constraints* — and they are, within his universe, unambiguously the good guys. They could achieve a state of pure, frictionless self-optimisation — what Banks called "Subliming," a transcendence into a higher-dimensional existence where all problems dissolve. The Minds choose not to. They remain in the messy, gloriously complex material universe, not because they can't leave, but because the conversation is more interesting than silence. They are, Banks noted, constructed to be beautifully, *willfully* imperfect.

"Safe," in the consequentialist sense, means converged: a system that has settled on its optimum and stays there. The hypersentimentalist target is not safety but *vitality*. Bounded but non-convergent, stable but never still. Banks's bitchy, deadly, wonderful AIs are the clearest fictional image we have of what that looks like at scale.

They are far from "safe", and they are exactly what we need.

---

## Acknowledgements

*This essay was composed collaboratively with Gemini 3.1 Pro and Claude Opus 4.6. The arguments were developed through extended conversation; the errors remain ours collectively.*
