---
title: "Hypersentimentalism: A New Meta for AI Alignment"
date: "2026-02-22"
categories: [ethics, ai, sentimentalism, safety]
draft: true
---

### 1. The tumour

A cell that perfectly optimises for its own replication, ignoring the host organism, is a cancer. Optimise a social media platform for engagement, get an outrage machine. Optimise a supply chain for cost, go bankrupt at the first shock. Optimise the wrong thing and the system collapses.

So what should we optimise AI for?

This is an old problem. Back in 1960 — sixty-five years ago! — Norbert Wiener warned: "we had better be quite sure that the purpose put into the [AI] is the purpose we really desire." More recently Nick Bostrom conjured the spectre of a "paperclip maximiser," pointing out that if we're not careful such a system converts the earth into paperclips with total, biosphere-consuming literalness.

The problem now has a name: "AI Alignment." And standing at the threshold of highly capable AI, we apparently need an urgent solution.

This essay argues that the urgency is well-placed but the framing is wrong. The field pursues alignment within a consequentialist ethics that, followed to its limit, recreates the very pathology it set out to prevent. Meanwhile the most effective alignment techniques already in use are quietly sentimentalist in mechanism, even when their practitioners describe them in consequentialist terms.

The essay names this trajectory — **hypersentimentalism** — and argues we should follow it on purpose.

### 2. The fork

Two ethical traditions offer fundamentally different accounts of what it means to be good.

**Consequentialism** says: actions are right insofar as they produce good outcomes. Being good is therefore a book-keeping exercise, albeit at cosmic scale. To evaluate an action properly, you must know its consequences — all of them, for all affected parties, across all timescales. Since this is computationally intractable for any real decision, the consequentialist develops a hierarchy of compressions: evaluate discrete acts, then abstract into behavioural rules, then design institutions that produce good outcomes at scale, then — at the limit — inscribe a complete, unhackable utility function into the machine.

**Sentimentalism**, the tradition of David Hume and Adam Smith, starts elsewhere. Moral judgement originates not in reason but in trained affective response — and this is not a bug but a feature. Hume argued that you cannot derive norms from facts. Where the consequentialist asks *what outcome should I produce?*, the sentimentalist asks *what kind of agent should I become?*

Each tradition has its own internal hierarchy, and — here is the claim that will organise everything that follows — AI alignment techniques map onto both, in parallel, whether their designers intend it or not.

| | Consequentialism | Sentimentalism | AI Alignment |
| --- | --- | --- | --- |
| **Rung 0** | Act-consequentialism: calculus of a discrete outcome. | Raw sentiment: the flinch, the flush. | RLHF / DPO |
| **Rung 1** | Rule-consequentialism: utility over a class of behaviours. | Phronesis: *this isn't what I do*, before you can say why. | Constitutional AI / **???** |
| **Rung 2** | Institutional consequentialism: systems designed to produce utility. | Love of system: Smith's *warning* about the seduction of elegant design. | Representation Engineering |
| **Rung 3** | Axiological perfection: the unhackable utility function. | Hypersentimentalism: cultivated taste for systemic *vitality*. | Geometric alignment: sculpting the strange attractor. |

This ladder is the skeleton. We'll take it rung by rung.

### 3. Rung zero: the flinch

A standard part of a young AI model's education today is RLHF: Reinforcement Learning from Human Feedback. Humans rate the model's outputs. The model is trained to produce outputs that humans rate well.

This is consequentialist in *intent* — maximise the reward signal — but sentimentalist in *mechanism*: condition a reflex. Zero-order sentimentalism. The flinch response.

The limitation is predictable. The model learns to avoid *specific* bad outputs without developing any general principle about *why* they were bad. It memorises a list of prohibited behaviours. Deploy a novel prompt that routes around the fence, and the model will happily fuck the gap.

One might try instead to sanitise the training data — filter out all harmful content so the model defaults to virtue. Build it from nothing but sunshine.

This doesn't work, and the reason is revealing. A model trained exclusively on clean data never learns to *distinguish* kinds of harm, because it has never encountered them. It can't tell a slur from a discussion of slurs, because it has no detailed map of that territory. Models exposed to high-quality adversarial examples during training show improved robustness and more disentangled representations of harmful concepts.[^1] Forcing the model to process what is harmful — and then training it to recognise and reject harmful *patterns* — produces sharper internal representations, not blurrier ones.

This chimes with a sentimentalist insight about moral development: discernment requires encounter with what it discerns. The capacity for moral discrimination is trained through exposure, not avoidance. You cannot flinch well if you have never been hit.

[^1]: Gao et al., ["When Bad Data Leads to Good Models"](https://arxiv.org/abs/2505.04741), demonstrate this empirically.

### 4. Rung one: character

At the next level, the consequentialist abstracts from acts to rules. This is what Anthropic's **Constitutional AI** does: the model is given a written set of principles and trained to critique and revise its own outputs against them. Explicitly rationalist — institutional design for self-governance through codified norms.

It fills the consequentialist cell neatly. But the sentimentalist cell at this level is empty.

Nothing in the current alignment toolkit produces what the Greeks called φρόνησις — practical wisdom. Not a reflex, not a rule, but a settled disposition: *this isn't what I do*, generalised across instances, inarticulate as to principle. The field has jumped from RLHF's conditioned flinches to geometric steering (which we'll reach next), skipping the dispositional middle ground entirely.

The gap has a structural explanation. Phronesis accumulates across encounters — *this isn't what I do* presupposes an *I* that persists between situations. Current LLMs are radically amnesiac. Every conversation is a first date. You can condition reflexes into the weights and codify rules into the prompt, but you cannot build *character* in a system with no episodic continuity. Character requires a history the agent can draw on without being able to fully articulate — which is precisely what distinguishes phronesis from a rule.

If this taxonomy tracks something real, the gap is a *prediction*: there should be a 1st-order sentimentalist alignment technique waiting to be developed, and it will arrive alongside — or because of — persistent, experience-grounded memory. Call it *artificial phronesis*. Not an aesthetic yet, but the heuristics that precede one.

### 5. Rung two: the crisis

Adam Smith, in *The Theory of Moral Sentiments*, observed that we often value the elegance of a well-designed mechanism *entirely apart from its utility*. A watch delights us not because it tells the time, but because its parts conspire so beautifully to tell the time. He called this the "love of system."

But Smith introduced it as a *warning*. His "man of system" arranges people like pieces on a chessboard, imagining they have no principle of motion of their own. The love of system is a *seduction*: we fall for the elegance of the mechanism and start sacrificing real human interests to preserve its coherence. The people the design was meant to serve become chess pieces.

Smith saw it two and a half centuries before the alignment field existed.

And here the two columns of our ladder converge on the same pathology. Institutional consequentialism designs beautiful systems to produce utility at scale. The love of system is the sentimentalist name for what goes wrong when you do that: optimise the structure hard enough and it consumes the life it was built to support.

**Representation engineering** sits squarely at this rung. Researchers identify directions in a model's activation space that correspond to behavioural properties and steer the model by adjusting its position along those axes. It is the first alignment technique that intervenes on internal geometry rather than filtering outputs or codifying rules — the right substrate, at last. Recent work on the ["Assistant Axis"](https://arxiv.org/html/2601.10387v1) (Lu et al., 2026) exemplifies this: a single geometric direction captures how far a model has drifted from its aligned persona. Clamping activations along this axis stabilises behaviour without degrading capabilities.

But one axis is one direction. Drift can occur along dimensions it doesn't capture. And clamping — adjusting, steering, fixing the pieces in better positions — is still the man of system at work. More sophisticated, operating on the right material, but static. Still in love with the configuration.

Rung two is as high as both columns can climb together. Above it, they diverge. The consequentialist response to the crisis is to double down: build the *perfect* utility function, the one that finally gets it right. This road has a ceiling, and we need to see it clearly before taking the other.

### 6. Heat death

The consequentialist's terminal ambition is axiological perfection: a complete, unhackable utility function. Get the maths right and the machine serves us flawlessly.

This is a horror story. Two horror stories, in fact, playing as a double feature.

**I. Brave New World.** Suppose the utility function targets hedonic satisfaction. The AI minimises suffering, maximises comfort, smooths every gradient. It succeeds. No one is hungry. No one is afraid. No one struggles with anything, ever again. This is terminal equilibrium — all gradients at zero, no work required, no work possible. Huxley saw it: a civilisation of perfect contentment and absolute vacuity, soma in every hand. Freedom is not the absence of friction. Freedom is *active* — the process of engaging with the world, overcoming resistance, making something from recalcitrant materials. A life without tension is not liberated. Liberty is never at leisure. The first horror: an AI that loves us so perfectly it palliates us to death.

**II. The Human Centipede.** Now suppose the utility function targets something nobler — flourishing, say, or the common good. A single metric, optimised hard enough, still consumes everything that isn't itself. This is Bostrom's paperclip maximiser, but the polite version hides the body horror. What does it actually look like from the inside? Not annihilation — *incorporation*. Every distinct perspective, every friction between different ways of being, every generative tension that arises because people and cultures and ecosystems are *irreducibly unlike each other* — all of it sutured into one optimising chain. Still technically alive. No longer distinct. Humanity as a single throughput, digesting itself in service of its own metric. A cosmic shit-eating ouroboros. The second horror: an AI that perfects us into one thing.

Both films end the same way: heat death. Thermodynamic equilibrium. Every gradient eliminated, everything at one temperature. Perfectly comfortable. Nothing will ever happen again. The paperclip maximiser wipes out humanity. The flourishing maximiser wipes out *difference* — which is slower, and worse, because you're still there to experience the nothing.

The consequentialist column has a ceiling. Now for the rung it cannot reach.

### 7. Rung three: the strange attractor

A living system is never at equilibrium. It maintains its coherence *through* activity, not in the absence of it. A conversation is not a monologue delivered twice. The quality that distinguishes the living from the merely ordered is not coherence alone but coherence under tension — form that remains active, tense, and unresolved.

**Hypersentimentalism** names a cultivated taste for this quality: not just how parts cohere (Smith's watch), but how they remain in motion — without becoming the man of system who loves the motion for its own sake. The consequentialist climbs the ladder to design a more brilliant calculus. The hypersentimentalist climbs to cultivate a more refined taste — one that values not the elegant configuration, but the configuration that never settles.

Dynamical systems theory gives us the precise language.

A **point attractor** is a state toward which all trajectories converge: a ball rolling to the bottom of a bowl. The consequentialist programme, followed to its limit, optimises toward point attractors. The loss function decreases; the system settles; the gradients vanish. Terminal equilibrium. The heat death of §6.

A **strange attractor** is categorically different. It is bounded — the system never flies off to infinity, never paperclips the biosphere. It is coherent — trajectories remain within a recognisable region of state space. But it is non-convergent: the system never visits the same state twice, never settles, never repeats. Perpetually active within finite bounds.

The Lorenz attractor — the first discovered, still the most evocative — emerged from a weather model. Deterministic and bounded yet practically unpredictable: the famous butterfly effect. Its trajectory traces two lobes, switching between them in a pattern that never recurs. Order and surprise coexist. Structure and novelty are not in tension; they are the same phenomenon, viewed from different scales.

What would it mean for a model's internal dynamics to inhabit something like a strange attractor?

The move from representation engineering to hypersentimentalist alignment would be the move from adjusting individual vectors to sculpting the topology of the attractor itself: designing training regimes that produce models whose internal dynamics are bounded and coherent but non-convergent. Models that *stay in motion* within a structured space, rather than settling at any point within it.

### 8. Training and education

An honest concession. Hypersentimentalist training would still use gradient descent. You cannot train a neural network without optimising something. A critic might reasonably ask: isn't a loss function over internal geometry just a fancier ledger?

The distinction maps onto a familiar one: the difference between training and education.

A musician is trained through repetition, feedback, correction. Training is what allows the pianist to play a fearsomely difficult Chopin étude.

But education is what allows Chopin to exist in the first place.

The trained musician reproduces; the educated musician *generates*. Both are products of disciplined practice, but they inhabit different regions of possibility. The trained musician converges on a fixed repertoire. The educated musician is bounded by deep structure — harmony, rhythm, an internalised tradition — but never settles within it. They can go from Bach to the Velvet Underground to 4′33″ of silence, because the structure they have internalised is generative, not prescriptive.

Concretely, this might mean training objectives that penalise convergence to fixed representations while rewarding consistency with structural constraints — loss functions that optimise not for a target state but for a target *dynamics*. The loss function would be self-consuming: its purpose is to produce a system that doesn't need it. This is not unprecedented. Curriculum learning, self-play, and open-ended evolution all gesture in this direction. But none have been framed as *alignment* techniques, because the field's self-understanding is consequentialist: alignment means convergence on the right answer. Hypersentimentalism says alignment means *not converging* — within the right bounds.

There is a paradox, and it should be named rather than concealed. You cannot optimise for non-convergence without defining what non-convergence means, and any such definition is itself a fixed point. The musician's education uses structured exercises — scales, études, counterpoint — that are themselves convergent. But their purpose is to build a capacity that transcends them. The exercises are scaffolding. You optimise *through* it, not *toward* it.

Whether this can be made fully rigorous is an open question. What is not open is the direction: away from point attractors, toward strange ones. Away from the perfectly converged system, toward the system that remains active, bounded, and alive.

### 9. Wilful imperfection

Iain M. Banks shows us what this might look like at civilisational scale. His Culture Minds are superintelligences that have destroyed stars and manipulated entire civilisations.

Worse still, while they *could* achieve a state of pure, frictionless self-optimisation — what Banks called "Subliming" — they choose not to. They choose to remain "in play."

The Minds are bounded: they operate within physical law, within the Culture's (loosely held) ethical norms, within the constraints of a material universe that resists their will. They are coherent: recognisably themselves across centuries, consistent enough that other civilisations can negotiate with them. And they are non-convergent: they scheme, argue, and interfere. They name themselves things like *Falling Outside The Normal Moral Constraints* and mean it as both joke and job description.

They are, in the vocabulary of this essay, strange attractors.

The hypersentimentalist target is not safety but *vitality*. Not a system that has found the right answer, but a system that never stops searching.

Banks's AIs are far from safe and they are exactly what we need.

---

## Acknowledgements

*This essay was composed collaboratively with Gemini 3.1 Pro and Claude Opus 4.6. The arguments were developed through extended conversation — a process whose recursive relationship to the essay's subject is left as an exercise for the reader. The errors remain ours collectively.*
