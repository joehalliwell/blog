[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joe Halliwell",
    "section": "",
    "text": "An analysis of clamp_with_grad()\n\n\n\n\n\n\n\npytorch\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/clamp-with-grad.html",
    "href": "posts/clamp-with-grad.html",
    "title": "An analysis of clamp_with_grad()",
    "section": "",
    "text": "This notebook examines the clamp_with_grad() function that’s sometimes used in the CLIP-guided art community.\nThe standard clamp() (AKA clip()) function has a zero gradient outside the clamped region. This is a problem for gradient descent optimizers.\nVarious people have proposed a fix for this: adding a new function with a pseudo-gradient. I understood the general idea of this, but not the specified, so set out to build some intuition about how the backward pass is defined and why the function works.\nWhat I discovered, however, was that this approach has a limitation: the gradient remains zero for negative values.\nFortunately it’s pretty easy to fix. I suggest an alternative definition which patches this, and demonstrate that we see improved behaviour."
  },
  {
    "objectID": "posts/clamp-with-grad.html#setup",
    "href": "posts/clamp-with-grad.html#setup",
    "title": "An analysis of clamp_with_grad()",
    "section": "Setup",
    "text": "Setup\nImport the usual libraries and tweak the matplotlib setup\n\nimport torch\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom math import pi\n\nFIGSIZE = 6\nmatplotlib.rcParams['figure.figsize'] = (FIGSIZE * 3, FIGSIZE)"
  },
  {
    "objectID": "posts/clamp-with-grad.html#plotting-the-derivative",
    "href": "posts/clamp-with-grad.html#plotting-the-derivative",
    "title": "An analysis of clamp_with_grad()",
    "section": "Plotting the derivative",
    "text": "Plotting the derivative\nTo look at the derivative, we define a helper function to compute the gradient at a point.\n\ndef get_gradient(f, xs):\n  xs = xs.detach().clone()\n  xs.requires_grad = True\n\n  ys = f(xs)\n  ys.backward(torch.ones_like(ys))  \n  return xs.grad.detach()\n\n\nx = torch.arange(-2, 2, 0.01)\n\nplt.plot(x, torch.sin(x * 5), label=\"sin(x)\")\nplt.plot(x, get_gradient(torch.sin, x * 5), label=\"dsin(x)/dx\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/clamp-with-grad.html#the-problem",
    "href": "posts/clamp-with-grad.html#the-problem",
    "title": "An analysis of clamp_with_grad()",
    "section": "The problem",
    "text": "The problem\nThe standard clamp functions has a zero gradient outside the clamped region. As a result gradient descent can’t do anything useful.\n\ndef plot(clamp_fn, ax, title):\n  x = torch.arange(-2, 2, 0.01)\n  clamp = lambda x: clamp_fn(x, -1, 1)\n  loss = lambda x: torch.sin(clamp(x) * pi) + 1\n\n  ax.plot(x, clamp(x), label=\"clamp\", alpha=0.5)\n  ax.plot(x, loss(x), label=\"loss\", alpha=0.5)\n  ax.plot(x, get_gradient(loss, x), label=\"dloss/dx\", alpha=0.5)\n    \n  ax.legend()\n  ax.title.set_text(title)\n\nfig, ax = plt.subplots(ncols=1, figsize=(FIGSIZE,FIGSIZE))\nplot(torch.clamp, ax, \"clamp\")\n\n\n\n\nIn this simple example, the minimum of the loss function (orange line) is at -0.5.\nHowever outside the \\([-1,1]\\) interval, the derivative of the loss function (green line) is zero."
  },
  {
    "objectID": "posts/clamp-with-grad.html#define-clamp_with_grad",
    "href": "posts/clamp-with-grad.html#define-clamp_with_grad",
    "title": "An analysis of clamp_with_grad()",
    "section": "Define clamp_with_grad()",
    "text": "Define clamp_with_grad()\nVarious folks have proposed the following approach to addressing this, by providing a pseudo-derivative outside the clamped interval:\n\nclass ClampWithGrad(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input, min, max):\n        ctx.min = min\n        ctx.max = max\n        ctx.save_for_backward(input)\n        return input.clamp(min, max)\n\n    @staticmethod\n    def backward(ctx, grad_in):\n        input, = ctx.saved_tensors\n        return (\n            grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) &gt;= 0),\n            None,\n            None,\n        )\n\nclamp_with_grad = ClampWithGrad.apply\n\n\nfig, ax = plt.subplots(ncols=2, figsize=(FIGSIZE*2, FIGSIZE))\nplot(torch.clamp, ax[0], \"clamp\")\nplot(clamp_with_grad, ax[1], \"clamp_with_grad\")"
  }
]